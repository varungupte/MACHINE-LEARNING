{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading only the columns containing categorical data from the input file along with the target column\n",
    "df2=pd.read_csv('train.csv',usecols=['sales','salary','Work_accident','promotion_last_5years','left'])\n",
    "#appending the 'left' column as the last column\n",
    "left=df2['left']\n",
    "df2=df2.drop(columns=['left'])\n",
    "df2=df2.join(left)\n",
    "#splitting the data for training and validation\n",
    "msk = np.random.rand(len(df2)) < 0.8\n",
    "train = df2[msk]\n",
    "test = df2[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy(df):\n",
    "    target = df.keys()[-1]   \n",
    "    entropy = 0\n",
    "    values = df[target].unique()\n",
    "    for value in values:\n",
    "        fraction = df[target].value_counts()[value]/(len(df[target])+eps)\n",
    "        entropy += -fraction*log(fraction+eps)\n",
    "    return entropy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy_attribute(df,attribute):\n",
    "    target = df.keys()[-1]   \n",
    "    #Will return the unique values present in the target column 'left'\n",
    "    target_variables = df[target].unique() \n",
    "    variables = df[attribute].unique()\n",
    "    entropy2 = 0\n",
    "    for variable in variables:\n",
    "        entropy = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(df[attribute][df[attribute]==variable][df[target] ==target_variable])\n",
    "            den = len(df[attribute][df[attribute]==variable])\n",
    "            fraction = num/(den+eps)\n",
    "            entropy += -fraction*log(fraction+eps)\n",
    "        fraction2 =(den)/len(df)\n",
    "        entropy2 += -fraction2*entropy\n",
    "    return abs(entropy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_winner(df):\n",
    "    Entropy_att = []\n",
    "    IG = []\n",
    "    for key in df.keys()[:-1]:       \n",
    "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
    "    return df.keys()[:-1][np.argmax(IG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtable(df, node,value):\n",
    "    return df[df[node] == value].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTree(df,tree=None): \n",
    "    \n",
    "    if len(df.columns)==1:\n",
    "        return df['left'].median()\n",
    "    Class = df.keys()[-1]   \n",
    "\n",
    "    #Get attribute with maximum information gain\n",
    "    node = find_winner(df)\n",
    "    \n",
    "    #Get distinct value of that attribute\n",
    "    attValue = np.unique(df[node])\n",
    "    \n",
    "    #Create an empty dictionary to create tree    \n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "\n",
    "    for value in attValue:\n",
    "        \n",
    "        subtable = get_subtable(df,node,value)\n",
    "        subtable=subtable.drop(columns=[node])\n",
    "        clValue,counts = np.unique(subtable['left'],return_counts=True)                        \n",
    "        \n",
    "        if len(counts)==1:#Checking purity of subset\n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = buildTree(subtable) #Calling the function recursively \n",
    "                   \n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to predict for any input variable \n",
    "def predict(inst,tree):\n",
    "    for nodes in tree.keys():        \n",
    "        \n",
    "        value = inst[nodes]\n",
    "        tree = tree[nodes][value]\n",
    "        prediction = 0\n",
    "            \n",
    "        if type(tree) is dict:\n",
    "            prediction = predict(inst, tree)\n",
    "        else:\n",
    "            return tree\n",
    "            break;                               \n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(data,tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predicted = pd.DataFrame(columns=['predicted'])\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"]=predict(queries[i],tree) \n",
    "    print accuracy_score(data['left'],predicted)\n",
    "    print confusion_matrix(data['left'],predicted)\n",
    "    print classification_report(data['left'],predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7507802050824788\n",
      "[[1683    0]\n",
      " [ 559    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86      1683\n",
      "           1       1.00      0.00      0.00       560\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      2243\n",
      "   macro avg       0.88      0.50      0.43      2243\n",
      "weighted avg       0.81      0.75      0.64      2243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#building tree and printing the tree formed. Testing the model over the 20% test data for validation.\n",
    "\n",
    "tree=buildTree(train)\n",
    "# import pprint\n",
    "# pprint.pprint(tree)\n",
    "\n",
    "#for validation, predicting the value of the test data(20% of the original data)\n",
    "testing(test,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing against test_sample.csv file\n",
    "def predictleft(tree,filename):\n",
    "    test_sample=pd.read_csv(filename)\n",
    "    left=test_sample['left']\n",
    "    test_sample=test_sample.drop(columns=['left'])\n",
    "    test_sample=test_sample.join(left)\n",
    "    test_sample\n",
    "    queries = test_sample.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    for i in range(len(test_sample)):\n",
    "        predict(queries[i],tree) \n",
    "    testing(test_sample,tree)\n",
    "    \n",
    "# predictleft(tree,'sample_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7728265618173875\n",
      "[[1768    0]\n",
      " [ 520    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87      1768\n",
      "           1       1.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      2289\n",
      "   macro avg       0.89      0.50      0.44      2289\n",
      "weighted avg       0.82      0.77      0.67      2289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparing result with in-built(scikit-learn) decision tree function to check correctness of algorithm used\n",
    "df=df2\n",
    "df\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "le_salary = LabelEncoder()\n",
    "le_sales=LabelEncoder()\n",
    "df['sales_n'] = le_salary.fit_transform(df['sales'])\n",
    "df['salary_n'] = le_sales.fit_transform(df['salary'])\n",
    "df=df.drop(['sales','salary'],axis='columns')\n",
    "\n",
    "#dividing the data into training and testing data(for validation)\n",
    "msk = np.random.rand(len(df2)) < 0.8\n",
    "train2 = df[msk]\n",
    "test2 = df[~msk]\n",
    "trainy=train2['left']\n",
    "trainx=train2.drop(['left'],axis='columns')\n",
    "\n",
    "#training the model\n",
    "model.fit(trainx,trainy)\n",
    "testy=test2['left']\n",
    "testx=test2.drop(['left'],axis='columns')\n",
    "\n",
    "#predicting over the test data\n",
    "pred=model.predict(testx)\n",
    "print accuracy_score(testy,pred)\n",
    "print confusion_matrix(testy,pred)\n",
    "print classification_report(testy,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
